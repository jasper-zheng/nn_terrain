[{
                "url": "/nn_terrain/tags/demos",
                "title": "Tag",
                "content": " demos "
            },{
                "url": "/nn_terrain/tags/tutorials",
                "title": "Tag",
                "content": " tutorials "
            },
                {
                    "url": "/nn_terrain/posts/conversation-with-keigo-yoshida/",
                    "title": "Latent space artists: A conversation with Keigo Yoshida",
                    "content": "In this series of interviews, we invited artists to share the design thinking and experience behind the making process of their demo projects. post demos We collaborated with several artist-researchers to explore NIME design with autoencoders and latent terrain. In this series of interviews, we invited artists to share the design thinking and experience behind the making process of their demo projects. The full annotated portfolio is at here.Artist BioKeigo Yoshida is a Tokyo-based artist and scientist who integrates neuroscience and computer science to explore novel forms of artistic expression. His notable works include the A/V performances &amp;quot;Propagation&amp;quot; and &amp;quot;Mineral Neurons&amp;quot;, both presented at Sónar+D 2025, and &amp;quot;liberated frequencies&amp;quot; (Installation and A/V performance). The latter work was supported by Flying Tokyo 2024, involved collaboration with METI and Rhizomatiks, and was also presented at IRCAM Forum Workshops 2025 Hors-les-Murs. https://keigoyoshida.jp/Repressive Latent Terrain: An experimental data sonification patchAn experimental data sonification patch designed by Keigo Yoshida. It allows the audience to engage in the experiment to explore the latent space through an OpenBCI Electroencephalography (EEG) headset. It aims to create a meditative listening experience, in which the audience is instructed to stay calm and relaxed, while the time-varying EEG data is sonified to a disruptive and arousal soundscape. The artistic theme is the active manipulation of consciousness in a focused meditation, and an adversarial tension between the subject&#39;s striving for calm and the algorithm&#39;s impulse toward arousal sonic responses.The EEG headset tracks two critical brainwave frequencies: the Alpha waves that are associated with deep relaxation and inner calm, and the Gamma waves that are associated with high-level arousal and active processing. These two time-varying frequencies are used as the control input for latent space navigation. In this way, the sonic expression continuously shifts based on the balance between how relaxed and how alert the subject is during meditation. The arrangement of latent trajectories is constantly changing, and therefore the terrain is constantly trained during the interaction.Could you tell us a bit about yourself and your artistic practice in MaxMSP / interactive sound art / music composition?Keigo: I’ve been using Max/MSP consistently in sound installations and audiovisual performances in my career. One example is my piece ‘liberated frequencies’, created for the Flying Tokyo 2024 project, which was supported by the Ministry of Economy, Trade and Industry (METI) and Rhizomatiks. For that work, I built a system that processed EEG brainwave data in Max/MSP and sent it to Python to control the latent space of a WaveNet model. This system was demonstrated at the IRCAM Forum Workshops in 2025 Hors-les-Murs. My practice extends to using Max/MSP and various other tools for sonic production in a wide range of projects.Before starting the nn.terrain project, did you have any prior hands-on experience with neural audio synthesis (e.g., RAVE, autoencoders), or more broadly AI music (e.g., text-to-audio, machine learning)?Keigo: My recent practice has focused heavily on integrating advanced neural networks into live performance. At Sónar+D 2025 AI Performance Playground, I presented two audiovisual performances: ‘Mineral Neurons’ used a neural synthesis (nn~) with gen_synth_fx coupled with a physical vibration system to transform acoustics in real-time. ‘Propagation’ used two instances of Netone’s RAVE-based Neutone morpho to convert raw noise signals into complex neural network noise.Keigo: I also worked on a Tedx KeioU conference, I collaborated with researchers Yuki Kawai and Shinya Fujii to integrate a reservoir computing model, which mimics the brain&#39;s learning process. The resulting A/V performance featured a dynamic call-and-response between a human drummer and the output of the reservoir computing model, showcasing a deeper form of machine-human interplay. These projects underscore my extensive practical engagement with machine learning across various creative domains.Is working with neural audio synthesis different from working with other materials (e.g., conventional sound synthesis algorithms / samples).Keigo: The possibilities of neural audio synthesis are unique due to their inherent learning process. For me the processes like interactively adapting the latent space to a terrain are how the system instantaneously traces and accesses past states (training data) in real-time to transform sound. They&#39;re similar to a sampler but distinct from it. Furthermore, because the model is able to shift timbre and musical structure across genres, I like to use completely different rhythmic patterns and sonic textures to experiment with the sound space creation. This opens creative possibilities that traditional synthesis or sample-based methods simply can’t offer.How did you integrate neural audio synthesis in your making/crafting process? What is your workflow?Keigo: I found useful about the approach of first mapping the latent space and then tracing the map like a musical score. This shares some commonalities with graphic scores, and the process of translating information into a visual form and treating it like a musical score is an entirely new compositional method to me.Keigo: This ability to read these scores in a multi-directional way rather than unidirectional, is very different from the traditional linearity embedded in score-writing."
                },
                {
                    "url": "/nn_terrain/posts/conversation-with-jiatong-liu/",
                    "title": "Latent space artists: A conversation with Jiatong Liu",
                    "content": "In this series of interviews, we invited artists to share the design thinking and experience behind the making process of their demo projects. post demos Interview with Jiatong Liu, edited by Jasper Shuoyang Zheng.We collaborated with several artist-researchers to explore NIME design with autoencoders and latent terrain. In this series of interviews, we invited artists to share the design thinking and experience behind the making process of their demo projects. The full annotated portfolio is at here.Artist BioJiatong Liu is a Creative Technologist studying at the Creative Coding Institute, University of the Arts London. Their practice and research revolve around Machine Learning, Composition and Film - combining narrative design, spatial design and technology. https://jiatongliu.com/nn/mémoire: An audio-visual installationnn/mémoire is an audio-visual virtual environment that aims to use neural audio synthesis to reimagine the sound of cultural heritage in Hutongs, a type of traditional northern Chinese courtyard house in Beijing. The piece is a virtual gallery that allows audiences to wander through and hear the real-time generated soundscape that captures the sound of Hutongs. The soundscape is composed of archival recordings from the Sound Art Museum Beijing, aiming to preserve the vanishing living culture that is now vanishing due to urban changes. It invites the audience to discover, remember, and connect to their memory of sound through the embodied listening in the environment.Could you tell us a bit about yourself and your artistic practice in MaxMSP / interactive sound art / neural audio synthesis?Jiatong: I’m a composer for film and theater with a background in creative music production. I also work on interactive sound projects that involve coding. In terms of Max/MSP, I’d say I’m an intermediate user—I’m comfortable with most objects and can prototype systems with a good level of independence.Jiatong: I didn’t have any prior experience with neural audio synthesis or AI music tools like RAVE or autoencoders. I hadn’t even heard of those terms before this project.Could you tell us about the work you did in the nn.terrain project?Jiatong: I created an interactive audio-visual installation that uses nn.terrain as a sound design tool inside Unreal Engine. The idea was to build a virtual sound environment where the audience explores a 3D game-like space in a first-person view. As they move through the environment, their movements are mapped to a neural network. This means that walking around changes the sounds they hear, which are generated from different parts of the latent space in the neural network.Jiatong: The theme of the piece is “nostalgic dreamcore,” and it uses traditional cultural sounds from Beijing hutongs. I worked with a dataset from the Sound Art Museum in Beijing, which archives disappearing hutong soundscapes. The goal was to recreate the feeling of these spaces in an immersive, interactive environment.Is working with neural audio synthesis different from working with conventional sound synthesis algorithms?Jiatong: Yes, it’s very different. Traditional music production tools—like reverb, compression, or EQ—mainly shape the sound that already exists. Neural audio synthesis, on the other hand, changes the structure and order of the sound itself. It feels closer to granular synthesis than to conventional techniques, but overall, it’s a completely new way of working compared to standard audio production.How did you approach composing with neural audio synthesis? What did you learn about neural audio synthesis through the making/crafting process of your project?Jiatong: Working with the 2D latent terrain map helped me understand how an autoencoder works internally—the encoder, decoder, and latent space architecture. To me, working with neural audio synthesis is a very different workflow than traditional materials, I had to find a balance between creating something new and experimental while keeping it somewhat familiar and recognizable for a broader audience.Jiatong: I need to learn to deal with the unpredictability of neural audio synthesis, through trial and error, I started to see patterns in how certain sounds are encoded and what unexpected results might emerge. I like being able to predict what would happen in the terrain, but also not so certain on all of them, is really interesting.Could you highlight some features of your work that make it valuable, unique, or special?Jiatong: One key feature is how the sound is mapped into a 3D Unreal Engine environment. I placed sounds in specific regions of the virtual space, which adds a spatial dimension to the experience. Another unique aspect is the use of Hutongs soundscapes from my hometown, which makes the work personal and culturally meaningful.What would you like to see implemented in the tool in the future?Jiatong: I’d love to see more flexible ways to draw and edit trajectories—something like a “Bezier curve” mode where you can sketch paths instead of clicking points manually. It would also be great to have options to delete or move latent vectors, similar to using an eraser or dragging objects in Photoshop. These features would make the composition process more intuitive, especially for live performance."
                },
                {
                    "url": "/nn_terrain/installation/",
                    "title": "Download and Installation",
                    "content": "Installing the nn.terrain~ externals to MaxMSP/Max4Live.  This repository is a set of MaxMSP/Max4Live externals to build, visualise, and program latent terrain:ObjectDescriptionnn.terrain~ Load, build, train, and save a terrain.Perform the coordinates-to-latents mapping. nn.terrain.encodeUsing a pre-trained autoencoder to encode audio buffers into latent trajectories, as training data for nn.terrain~.nn.terrain.recordManually recording latent trajectories, as training data for nn.terrain~.nn.terrain.guiEdit spatial trajectories, as training data for nn.terrain~.Visualise the terrain.Create and program trajectory playbacks.All externals above are designed to work together with nn~, which is a MaxMSP/Max4Live external developed by acids-ircam to load and use deep learning realtime audio processing models:ObjectDescriptionnn~ Hosting the decoder of an AI audio autoencoder, to decode latents to audio in realtime. DownloadDepending on which version of nn~ you&#39;re using:(Recommended) If you&#39;re using nn~ v1.5.6 (torch v2.0.x) (the 2023 version): Please download nn.terrain~ v1.5.6.If you&#39;re using nn~ v1.6.0 (torch v2.5.x) (the 2025 version): Please download nn.terrain~ v1.6.0.If you&#39;re unsure which version you&#39;re using, this can be checked from your Max console when an nn~ instance is first opened in Max:If you have a self-compiled combination like me, you might need to consider compiling your own nn.terrain~ from source, please see instructions Compile from Source.Installationnn.terrain~ needs to be installed in different ways, depending on which version of nn~ you&#39;re using:With nn~ v1.5.6macOSUncompress the .tar.gz file into the Package folder of your Max documents, which is usually in ~/Documents/Max 9/Packages.Reopen Max and you can find all nn.terrain objects.You might get a quarantine warning, proceed will disable this warning.WindowsUncompress the .tar.gz file into the Package folder of your Max documents, which is usually in ~/Documents/Max 9/Packages/Copy all .dll files in the package next to the Max.exe executable (if you have already done this for nn~, you don&#39;t need to do this again).With nn~ v1.6.0macOSMake sure that you have nn_tilde v1.6.0 installed,Uncompress the .tar.gz file,In ~/Documents/Max 9/Packages, copy and merge the unzipped folders to the nn_tilde folder, make sure that all externals (e.g., nn.terrain~.mxo) are placed next to nn~.mxo in the same folder.Reopen Max and you will find all nn.terrain objects. You might get a quarantine warning, proceed will disable this warning.WindowsMake sure that you have nn_tilde v1.6.0 installed,Uncompress the .tar.gz file,In ~/Documents/Max 9/Packages, copy and merge the unzipped folders to the nn_tilde folder, make sure that all externals (e.g., nn.terrain~.mxe64) are placed next to nn~.mxe64 in the same folder.Reopen Max and you will find all nn.terrain objects. You might get a quarantine warning, proceed will disable this warning.If the externals have trouble opening in Max, or doesn&#39;t work correctly with nn~ you might considering compiling the externals yourself, see Compile from Source."
                },
                {
                    "url": "/nn_terrain/instructions/",
                    "title": "Instructions",
                    "content": "Some instructions for getting started exploring AI audio generation with latent terrain.  OverviewUsing Pre-Trained TerrainsCreating VisualisationsBuilding Terrains from ScratchRecording Latents TrajectoriesComposing and Interacting with Terrains"
                },
                {
                    "url": "/nn_terrain/pre-trained/",
                    "title": "Pre-Trained Terrains (Presets)",
                    "content": "Some already built terrains to get started with.  Make sure to have both:an autoencoder (a .ts file)a terrain model (a .pt file).Autoencoder: Percussion - RAVE v1Pre-trained autoencoder author: Antoine CaillonDownload from RAVE official repository percussion.ts.Terrain: percussion-terrain-demo.ptConfig: 2 inputs 4 outputs, range: -16, -4, 16, 4Autoencoder: Music2Latent-ScriptedOriginal model is the work by Marco Pasini at SonyCSLParis.Scripted model can be downloaded from music2latent.ts.Terrain: m2l-demo.pt (the one used in the demo video)Training data from: Stem from an EDM track I made under the name Alaska Winter.Config: 2 inputs 64 outputs, range: -16, -4, 16, 4Autoencoder: Stable Audio Open 1.0Pre-trained model by Stability AI.A streamable scripted version for realtime continuous inference in MaxMSP: jasper-zheng/streamable-stable-audio-open.Terrain: stable-audio-terrain-demo.ptConfig: 2 inputs 64 outputs, range: -8, -8, 8, 8"
                },
                {
                    "url": "/nn_terrain/compile/",
                    "title": "Compile from Source",
                    "content": "Building nn.terrain~ externals from the source code.  If the externals have trouble opening in Max, or doesn&#39;t work correctly with nn_tilde, you might want to build the externals yourself:PrerequisitesMacOS:Xcode 11 or 12 (you can get from the App Store for free).Download LibTorch here and unzip it to a known directory. LibTorch&#39;s torch version should be the same as nn_tilde.Install a recent version of CMake (version 3.19 or higher).Windows:Download LibTorch here and unzip it to a known directory. LibTorch&#39;s torch version should be the same as nn_tilde.If you would like to enable GPU training/inference, you&#39;ll need to select the CUDA version of LibTorch, and have the corresponding CUDA ToolKit.Install a recent version of CMake (version 3.19 or higher).Build on MacOSRecursively clone this repository into Max&#39;s Packages folder. Terminal command:git clone https://github.com/jasper-zheng/nn_terrain.git --recursiveIn Terminal, cd into the nn_terrain folder you cloned, and make a new folder named build. and cd into that folder:cd nn_terrainmkdir buildcd buildRun the command below to generate an Xcode project, replace path/to/libtorch to the libtorch folder you&#39;ve downloaded:cmake ../src/ -G Xcode -DCMAKE_PREFIX_PATH=/path/to/libtorch -DCMAKE_BUILD_TYPE=Release -DCMAKE_OSX_ARCHITECTURES=arm64 An Xcode project will be created in this build folder, you can either open the Xcode project and build it from there, or build by running the command below:cmake --build .The .mxo files will be created in the src/externals folder, move them ~/Documents/Max 9/Packages/nn_terrain/externals/Additionally, taken from min-devkit:If you are running on a Mac with Apple Silicon, you might see an error cannot be loaded due to system security policy when loading your externals in Max. To resolve this, you can ad-hoc codesign your external with codesign --force --deep, this can be added in Xcode &amp;quot;Build Settings&amp;quot; page, in the &amp;quot;Signing&amp;quot; section, in &amp;quot;Other Code Signing Flags&amp;quot;.Build on WindowsRecursively clone this repository into Max&#39;s Packages folder. Terminal command:git clone https://github.com/jasper-zheng/nn_terrain.git --recurse-submodulesIn Terminal, cd into the nn_terrain folder you cloned, and make a new folder named build. and cd into that folder:cd nn_terrainmkdir buildcd buildThen run the command below to generate a project buildsystem, replace pathtolibtorch to the libtorch folder you&#39;ve downloaded, and make sure Visual Studio 17 2022 is set to your build system generator (run cmake --help to get a list of available options).cmake ..src -A x64 -DCMAKE_PREFIX_PATH=&amp;quot;pathtolibtorch&amp;quot; -G &amp;quot;Visual Studio 17 2022&amp;quot;Having generated the projects, now you can build by opening the .sln file in Visual Studio, or build on the command line using:cmake --build . --config ReleaseThe externals will be created in the src/externals folder, move them ~/Documents/Max 9/Packages/nn_terrain/externals/"
                },
                {
                    "url": "/nn_terrain/",
                    "title": "Latent Terrain Synthesis",
                    "content": "Building new musical instruments that compose and interact with AI audio generators.  New MaxMSP externals updates: v1.5.6.1-oct2025, which added support for stable-audio-open-1.0 autoencoder, example MaxMSP/Max4Live devices will be available soon.WelcomeLatent terrain is a tool to build corpus-based sound spaces/maps/materials to steer neural audio autoencoders/codecs (such as RAVE, Stable Audio Open (codec), Music2Latent). A terrain is a surface map for the autoencoder&#39;s latent space, taking coordinates in a control space as inputs, and producing continuous real-time latent trajectories that can be used for sound synthesis.Latent terrain aims to open up the creative possibilities of latent space navigation, allowing one to adapt the latent space of an autoencoder to easy-to-navigate interfaces (such as gestural controllers, stylus and tablets, XY-pads, and more), explore it like walking on a terrain surface, and build new musical instruments that compose and interact with AI audio generators.An example latent space walk with Music2Latent:Example applicationsSteering a neural audio autoencoder (tutorial coming soon).Building 1D/2D latent granular synthesiser (tutorial coming soon).Latent looping device.Supported autoencodersLatent terrain can work with any audio autoencoder as long as it offers latent variables. However, only a limited number of them have been implemented for MaxMSP, and we have only tested the following models:RAVE Realtime Audio Variational autoEncoder for fast and high-quality neural audio synthesis, by Antoine Caillon and Philippe Esling.Music2Latent-Scripted Music2Latent is a Consistency Autoencoder to encode and decode audio samples, by Marco Pasini, Stefan Lattner, and George Fazekas. We&#39;re using a scripted fork of the original repository.Stable Audio Open 1.0 (autoencoder) [Only supported by nn.terrain oct-2025 version] The pretransform audoencoder in Stable Audio Open 1.0 (not the text-to-audio diffusion model, only the audoencoder).We plan to test the following model in the future:FlowDecvschaos2Get startedDownload and InstallationInstructionsPre-Trained Terrains (Presets)Compiling from SourceGet in touchHi, this is Shuoyang (Jasper). nn.terrain~ is part of my ongoing PhD work on Discovering Musical Affordances in Neural Audio Synthesis, supervised by Anna Xambó Sedó and Nick Bryan-Kinns, and part of the work has been (will be) on putting AI audio generators into the hands of composers/musicians.Therefore, I would love to have you involved in it - if you have any feedback, a features request, a demo / a device / or anything made with nn.terrain, I would love to hear. If you would like to collaborate on anything, please leave a message in this feedback form.AcknowledgementsShuoyang Zheng, the author of this work, is supported by the UKRI Centre for Doctoral Training in Artificial Intelligence and Music [EP/S022694/1]."
                },
                {
                    "url": "/nn_terrain/instructions/overview/",
                    "title": "Overview",
                    "content": "What is neural audio autoencoder and why using latent terrain?  A neural audio autoencoder (such as RAVE) is an AI audio generation tool, it has two components: an encoder and a decoder.The encoder compresses a piece of audio signal into a sequence of latent vectors (a latent trajectory). This compression happens in the time domain, so that the sampling rate goes from 44100Hz (audio sampling rate) to 21.5Hz (latent space sampling rate).The decoder takes the latent trajectory to produce a piece of audio signal. The decoder can also be used as a parametric synthesiser by navigating the latent space (i.e., latent space walk).A latent terrain is a surface map created from latent trajectories, see the screenshot below, it&#39;s like looking at a latent trajectory from above. A latent trajectory can be generated by pathing through the terrain surface. Therefore, sound synthesis is achieved by sampling across the terrain to produce latent trajectories, which are then passed through the decoder to reconstruct waveform. Audio sample, latent trajectories, and latent terrain. Latent terrain aims to be nonlinear (i.e., able to produce complex sequential patterns), continuous (i.e., allows for smooth interpolations), and tailorable (i.e., DIY your own materials with interactive machine learning).You may have seen &amp;quot;latent space walk&amp;quot; animations created by AI image generators, for instance, this is a latent space walk in a diffusion transformer:Credit: https://github.com/rnbwdsh/ComfyUI-LatentWalkIn parallel, this is an latent space walk with Music2Latent, and we&#39;re using the mouse (stylus) to control the sampling position in a terrain:"
                },
                {
                    "url": "/nn_terrain/instructions/load-terrain/",
                    "title": "Using pre-trained terrains",
                    "content": "Feeling like using some already built terrains? They can be loaded like presets. tutorials This video introduces loading a terrain with 2 continuous control channels, navigating an autoencoder with 4 latent dimensions.What we needMake sure to have nn.terrain package and nn~ installed correctly, see Download and Installation for this.Make sure to grab an autoencoder (a .ts file) and a terrain model (a .pt file) from Pre-Trained Terrains (Presets).Synopsis Example can be found in the help file of `nn.terrain`. This tutorial guides you through:A brief introduction of an autoencoder and latent terrain,Loading a pre-trained terrain in MaxMSP,Adding an XY pad to control the navigation in the terrain,Visualising latent vectors (via multisliders),Visualising the terrain (rendering a surface map).Video:"
                },
                {
                    "url": "/nn_terrain/instructions/visualisation/",
                    "title": "Creating visualisations",
                    "content": "Given a terrain with 2D inputs, we can visualise it as a map.  Since the control space is 2D, the latent space can be visualised by sampling the control space across a closed interval (i.e., width and height in this example). Use the plot_interval message to do this:plot_interval for 2D plane takes 6 arguments:interval value boundary at the left, top, right, bottom (these usually are the same as the values_bound attribute of nn.terrain.gui), and the resolution of the x and y axes (these usually are the same as the width and height of nn.terrain.gui)The greyscale visualisation is like a surface map that looks at a terrain from above, where brighter pixels denote higher latent value."
                },
                {
                    "url": "/nn_terrain/instructions/build-terrain/",
                    "title": "Building terrains from scratch",
                    "content": "Instructions on building and training a customised terrain using supervised machine learning. tutorials An object nn.terrain~ 2 4 creates an empty terrain with 2 control channels and 4 latent channels. The video tutorial introduces creating training dataset for building nn.terrain~ 2 4.What we needTo build a terrain from scratch, we need a training dataset: pairs of latent trajectories and spatial trajectories.Latent trajectories are sequences of latent vectors encoded from audio buffers.Spatial trajectories are sequences of coordiantes in a control space. For instance:Trajectory of mouse in an XY track padTrajectory of hand gestures in an XYZ 3D spaceTimestamps in a timeline playback systemA terrain is a supervised machine learning model that learns this coordiantes-to-latents pairs, to produce new latent vectors given any coordiantes in the control space, so that the control space can be rendered as a &amp;quot;map&amp;quot; for the latent space. Latent trajectories, spatial trajectories, and latent terrain. Synopsis Example can be found in the help file of `nn.terrain`. This tutorial guides you through:Creating an empty terrain with customised number of inputs, outputs.Gathering latent trajectories from audio buffers, using nn.terrain.encode.Plotting 2D spatial trajectories in nn.terrain.gui with customised length.Training the empty terrain with latent and spatial trajectories.Setting up training datasetMonitoring the training lossVisualising terrain on-the-flySaving checkpointsVideo:"
                },
                {
                    "url": "/nn_terrain/instructions/record-latents/",
                    "title": "Recording trajectories",
                    "content": "Recording latents or spatial trajectories as training dataset to build a terrain. tutorials The nn.terrain.record object is a recorder that record multichannel signal as dictionary dataset. The recorded dictionary can be used to build a latent terrain.The video tutorial covers building a terrain in a 3D control space, for an autoencoder (music2latent). The exported music2latent.ts can be foundSynopsis Example can be found in the help file of `nn.terrain.record`. This tutorial guides you through:Creating an empty terrain with 3 inputs, 64 outputs,Scripting patch cords (i.e., adding all cords in one click),Recording latent trajectory with an encoder, from an audio buffer,Recording spatial trajectory from three sliders,Training the terrain.Video:"
                },
                {
                    "url": "/nn_terrain/instructions/interact/",
                    "title": "Composing and interacting with a terrain",
                    "content": "Programming trajectories in a 2D terrain.  The play mode of nn.terrain.gui offers position-based trajectory playback, which is similar to the play~ object in Max - plays back latent trajectories based on an offset within the trajectory.It can be used with any signal that generates a positional value in milliseconds (e.g., the line~ object).Please see the help file of nn.terrain.gui in Max for details:"
                },
                {
                    "url": "/nn_terrain/BuildInstructions/",
                    "title": "./BuildInstructions.md",
                    "content": "  Build InstructionsIf the externals have trouble opening in Max, or doesn&#39;t work correctly with nn_tilde, you might want to build the externals yourself:PrerequisitesMacOS (arm64):Xcode 11 or 12 (you can get from the App Store for free).Download arm64 LibTorch here and unzip it to a known directory. LibTorch&#39;s torch version should be the same as nn_tilde.Install a recent version of CMake (version 3.19 or higher).Windows:Download LibTorch here and unzip it to a known directory. LibTorch&#39;s torch version should be the same as nn_tilde.If you would like to enable GPU training/inference, you&#39;ll need to select the CUDA version of LibTorch, and have the corresponding CUDA ToolKit.Install a recent version of CMake (version 3.19 or higher).Build on MacOSRecursively clone this repository into Max&#39;s Packages folder. Terminal command:git clone https://github.com/jasper-zheng/nn_terrain.git --recursiveIn Terminal, cd into the nn_terrain folder you cloned, and make a new folder named build. and cd into that folder:cd nn_terrainmkdir buildcd buildRun the command below to generate an Xcode project, replace path/to/libtorch to the libtorch folder you&#39;ve downloaded:cmake ../src/ -G Xcode -DCMAKE_PREFIX_PATH=/path/to/libtorch -DCMAKE_BUILD_TYPE=Release -DCMAKE_OSX_ARCHITECTURES=arm64 An Xcode project will be created in this build folder, you can either open the Xcode project and build it from there, or build by running the command below:cmake --build .The .mxo files will be created in the src/externals folder, move them ~/Documents/Max 9/Packages/nn_terrain/externals/Additionally, taken from min-devkit:If you are running on a Mac with Apple Silicon, you might see an error cannot be loaded due to system security policy when loading your externals in Max. To resolve this, you can ad-hoc codesign your external with codesign --force --deep -s - myobject.mxo.Build on WindowsRecursively clone this repository into Max&#39;s Packages folder. Terminal command:git clone https://github.com/jasper-zheng/nn_terrain.git --recurse-submodulesIn Terminal, cd into the nn_terrain folder you cloned, and make a new folder named build. and cd into that folder:cd nn_terrainmkdir buildcd buildThen run the command below to generate a project buildsystem, replace pathtolibtorch to the libtorch folder you&#39;ve downloaded, and make sure Visual Studio 17 2022 is set to your build system generator (run cmake --help to get a list of available options).cmake ..src -A x64 -DCMAKE_PREFIX_PATH=&amp;quot;pathtolibtorch&amp;quot; -G &amp;quot;Visual Studio 17 2022&amp;quot;Having generated the projects, now you can build by opening the .sln file in Visual Studio, or build on the command line using:cmake --build . --config ReleaseThe externals will be created in the src/externals folder, move them ~/Documents/Max 9/Packages/nn_terrain/externals/"
                },
                {
                    "url": "/nn_terrain/posts/",
                    "title": "./core/libdoc_blog.liquid",
                    "content": "  2025-12-15 Jiatong Liu Latent space artists: A conversation with Jiatong Liu In this series of interviews, we invited artists to share the design thinking and experience behind the making process of their demo projects. 2025-12-05 Keigo Yoshida Latent space artists: A conversation with Keigo Yoshida In this series of interviews, we invited artists to share the design thinking and experience behind the making process of their demo projects. 1"
                },
                {
                    "url": "/nn_terrain/tags/",
                    "title": "./core/libdoc_tags.liquid",
                    "content": "  demos tutorials"
                }
]